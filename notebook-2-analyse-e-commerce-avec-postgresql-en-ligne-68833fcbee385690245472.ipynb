{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I43NxonzOSDg"
      },
      "source": [
        "# Notebook 2 - SQL avec vraies bases de donn√©es\n",
        "## Analyse e-commerce avec PostgreSQL en ligne\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JItQV6o4Ojrm"
      },
      "source": [
        "\n",
        "### üéØ Objectifs p√©dagogiques\n",
        "- Connecter Python √† une vraie base de donn√©es PostgreSQL\n",
        "- √âcrire des requ√™tes SQL complexes sur des donn√©es r√©elles\n",
        "- Impl√©menter des analyses RFM avec SQL\n",
        "- Int√©grer SQL et pandas pour des analyses avanc√©es\n",
        "- G√©rer les connexions et la s√©curit√©\n",
        "\n",
        "### üõçÔ∏è Contexte du projet\n",
        "Vous analysez les donn√©es d'un vrai dataset e-commerce (Brazilian E-Commerce Public Dataset) h√©berg√© sur une base PostgreSQL.\n",
        "\n",
        "Objectif : cr√©er une segmentation client√®le pour optimiser les campagnes marketing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K79TBMVvOuoj"
      },
      "source": [
        "## Partie 1 : Connexion √† la base de donn√©es r√©elle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mq7n18iwPBPe"
      },
      "source": [
        "### üîß Installation et configuration\n",
        "\n",
        "\n",
        "# Installation des d√©pendances\n",
        "\n",
        "\n",
        "```\n",
        "pip install psycopg2-binary sqlalchemy pandas python-dotenv\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_NuY2FHuOhu3"
      },
      "outputs": [],
      "source": [
        "import psycopg2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sqlalchemy import create_engine\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import os\n",
        "from sqlalchemy import text\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEbORVz5PXMa"
      },
      "source": [
        "### üåê Base de donn√©es PostgreSQL gratuite (ElephantSQL)\n",
        "\n",
        "**Option 1 : ElephantSQL (20MB gratuit)**\n",
        "1. Cr√©ez un compte sur [elephantsql.com](https://www.elephantsql.com/)\n",
        "2. Cr√©ez une instance \"Tiny Turtle\" (gratuite)\n",
        "3. R√©cup√©rez vos credentials\n",
        "\n",
        "**Option 2 : Supabase (500MB gratuit)**\n",
        "1. Cr√©ez un compte sur [supabase.com](https://supabase.com/)\n",
        "2. Cr√©ez un nouveau projet\n",
        "3. R√©cup√©rez l'URL de connexion PostgreSQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "ytLvCF3fQxRJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connexion Ok | Version :  PostgreSQL 17.4 on aarch64-unknown-linux-gnu, compiled by gcc (GCC) 13.2.0, 64-bit\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Configuration de connexion (√† adapter selon votre provider)\n",
        "DATABASE_CONFIG = {\n",
        "    'host': 'aws-0-eu-west-3.pooler.supabase.com',  #  host Supabase\n",
        "    'database': 'postgres',\n",
        "    'user': 'postgres.najrxlrneiiweukjgyea',\n",
        "    'password': '0000aaa111ZZZ!?',\n",
        "    'port': 5432,\n",
        "}\n",
        "\n",
        "\n",
        "# Cr√©ation de l'engine SQLAlchemy\n",
        "engine = create_engine(\n",
        "    f\"postgresql://{DATABASE_CONFIG['user']}:{DATABASE_CONFIG['password']}@\"\n",
        "    f\"{DATABASE_CONFIG['host']}:{DATABASE_CONFIG['port']}/{DATABASE_CONFIG['database']}\"\n",
        ")\n",
        "\n",
        "# Test de connexion\n",
        "def test_connection():\n",
        "    \"\"\"\n",
        "    Testez votre connexion √† la base\n",
        "\n",
        "    √âtapes :\n",
        "    1. Utilisez pd.read_sql() pour ex√©cuter \"SELECT version()\"\n",
        "    2. Affichez la version PostgreSQL\n",
        "    3. G√©rez les erreurs de connexion\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_sql('SELECT version();', engine)\n",
        "        print('Connexion Ok | Version : ', df.iloc[0,0])\n",
        "    except Exception as e:\n",
        "        print(f\"Erreur de connexion : {e}\")\n",
        "        return False\n",
        "    return True\n",
        "test_connection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXfOgAxGQ3b5"
      },
      "source": [
        "\n",
        "## Partie 2 : Import du dataset e-commerce\n",
        "\n",
        "### üìä Dataset Brazilian E-Commerce\n",
        "Nous utilisons le c√©l√®bre dataset Olist (100k commandes r√©elles).\n",
        "\n",
        "**Tables √† cr√©er :**\n",
        "1. **customers** : customer_id, customer_city, customer_state\n",
        "2. **orders** : order_id, customer_id, order_status, order_date, order_delivered_date\n",
        "3. **order_items** : order_id, product_id, seller_id, price, freight_value\n",
        "4. **products** : product_id, product_category, product_weight_g\n",
        "5. **sellers** : seller_id, seller_city, seller_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2_uVipWkQ_W8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "olist_order_payments_dataset.csv charg√© : 103886 lignes, 5 colonnes\n",
            "olist_order_reviews_dataset.csv charg√© : 99224 lignes, 7 colonnes\n",
            "olist_products_dataset.csv charg√© : 32951 lignes, 9 colonnes\n",
            "olist_customers_dataset.csv charg√© : 99441 lignes, 5 colonnes\n",
            "olist_sellers_dataset.csv charg√© : 3095 lignes, 4 colonnes\n",
            "olist_orders_dataset.csv charg√© : 99441 lignes, 8 colonnes\n",
            "olist_geolocation_dataset.csv charg√© : 1000163 lignes, 5 colonnes\n",
            "product_category_name_translation.csv charg√© : 71 lignes, 2 colonnes\n",
            "olist_order_items_dataset.csv charg√© : 112650 lignes, 7 colonnes\n"
          ]
        }
      ],
      "source": [
        "### üì• Import des donn√©es via API\n",
        "\n",
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "\n",
        "# def download_olist_dataset():\n",
        "#     \"\"\"\n",
        "#     T√©l√©charge le dataset Olist depuis Kaggle API\n",
        "\n",
        "#     Alternative : Utilisez l'API publique de l'IBGE (Institut br√©silien)\n",
        "#     pour des donn√©es e-commerce synth√©tiques mais r√©alistes\n",
        "#     \"\"\"\n",
        "\n",
        "#     # URL des donn√©es publiques br√©siliennes\n",
        "#     IBGE_API = \"https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce\"\n",
        "\n",
        "#     # R√©cup√©ration des donn√©es de villes (pour la g√©olocalisation)\n",
        "#     cities_url = f\"{IBGE_API}/localidades/municipios\"\n",
        "\n",
        "#     try:\n",
        "#         response = requests.get(cities_url)\n",
        "#         cities_data = response.json()\n",
        "\n",
        "#         # Convertir en DataFrame\n",
        "#         cities_df = pd.DataFrame(cities_data)\n",
        "\n",
        "#         # Votre code pour nettoyer et structurer\n",
        "#         # Cr√©ez des donn√©es e-commerce r√©alistes bas√©es sur ces villes\n",
        "\n",
        "#         return cities_df\n",
        "#     except Exception as e:\n",
        "#         print(f\"Erreur API IBGE : {e}\")\n",
        "#         return None\n",
        "#     pass\n",
        "\n",
        "# G√©n√©ration de donn√©es e-commerce r√©alistes\n",
        "# def generate_ecommerce_data(cities_df, n_customers=10000):\n",
        "#     \"\"\"\n",
        "#     G√©n√®re des donn√©es e-commerce r√©alistes\n",
        "\n",
        "#     √âtapes guid√©es :\n",
        "#     1. S√©lectionnez 50 villes br√©siliennes al√©atoirement\n",
        "#     2. Cr√©ez des clients avec distribution r√©aliste\n",
        "#     3. G√©n√©rez des commandes avec saisonnalit√©\n",
        "#     4. Ajoutez des produits avec cat√©gories coh√©rentes\n",
        "#     5. Calculez des prix et frais de port bas√©s sur la distance\n",
        "#     \"\"\"\n",
        "#     pass\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "olist_order_payments_dataset.csv charg√© : 103886 lignes, 5 colonnes\n",
            "olist_order_reviews_dataset.csv charg√© : 99224 lignes, 7 colonnes\n",
            "olist_products_dataset.csv charg√© : 32951 lignes, 9 colonnes\n",
            "olist_customers_dataset.csv charg√© : 99441 lignes, 5 colonnes\n",
            "olist_sellers_dataset.csv charg√© : 3095 lignes, 4 colonnes\n",
            "olist_orders_dataset.csv charg√© : 99441 lignes, 8 colonnes\n",
            "olist_geolocation_dataset.csv charg√© : 1000163 lignes, 5 colonnes\n",
            "product_category_name_translation.csv charg√© : 71 lignes, 2 colonnes\n",
            "olist_order_items_dataset.csv charg√© : 112650 lignes, 7 colonnes\n"
          ]
        }
      ],
      "source": [
        "# Mise en DataFrame des csv\n",
        "# Liste pour stockers les dataframes\n",
        "df_list = {}\n",
        "# Dossier avec les csv\n",
        "data_folder = \"data\"\n",
        "# Boucle sur les fichiers du dossier \n",
        "for csv in os.listdir(data_folder):\n",
        "    if csv.endswith(\".csv\"): # Filtre les fichiers au format csv\n",
        "        file_path = os.path.join(data_folder, csv)\n",
        "        df = pd.read_csv(file_path) # Lecture des csv\n",
        "        # Stockage dans le dictionnaire df_list\n",
        "        name = os.path.splitext(csv)[0]\n",
        "        df_list[name] = df\n",
        "        print(f'{csv} charg√© : {df.shape[0]} lignes, {df.shape[1]} colonnes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Tables √† cr√©er :**\n",
        "1. **customers** : customer_id, customer_city, customer_state\n",
        "2. **orders** : order_id, customer_id, order_status, order_date, order_delivered_date\n",
        "3. **order_items** : order_id, product_id, seller_id, price, freight_value\n",
        "4. **products** : product_id, product_category, product_weight_g\n",
        "5. **sellers** : seller_id, seller_city, seller_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "GqGIXooNSTjp"
      },
      "outputs": [],
      "source": [
        "### üóÉÔ∏è Cr√©ation des tables SQL\n",
        "def create_tables():\n",
        "    \"\"\"\n",
        "    Cr√©ez les tables dans PostgreSQL\n",
        "\n",
        "    Tips :\n",
        "    - Utilisez des SERIAL pour les IDs auto-increment\n",
        "    - Ajoutez des index sur les cl√©s √©trang√®res\n",
        "    - Incluez des contraintes de validation\n",
        "    \"\"\"\n",
        "\n",
        "    create_customers = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS customers (\n",
        "        customer_id SERIAL PRIMARY KEY,\n",
        "        customer_city VARCHAR(100),\n",
        "        customer_state VARCHAR(2)\n",
        "    );\n",
        "    \"\"\"\n",
        "    create_orders = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS orders (\n",
        "        order_id SERIAL PRIMARY KEY,\n",
        "        customer_id INT NOT NULL,\n",
        "        order_status VARCHAR(20) CHECK (order_status IN (\n",
        "            'created', 'shipped', 'delivered', 'canceled'\n",
        "        )),\n",
        "        order_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
        "        order_delivered_date TIMESTAMP,\n",
        "        CONSTRAINT fk_customer\n",
        "            FOREIGN KEY(customer_id)\n",
        "                REFERENCES customers(customer_id)\n",
        "        );\n",
        "    \"\"\"\n",
        "    create_order_items = \"\"\" \n",
        "    CREATE TABLE IF NOT EXISTS order_items (\n",
        "        order_id SERIAL PRIMARY KEY,\n",
        "        product_id INT NOT NULL,\n",
        "        seller_id INT,\n",
        "        price NUMERIC(10,2) CHECK (price >= 0),\n",
        "        freight_value NUMERIC(10,2) CHECK (freight_value >= 0),\n",
        "        CONSTRAINT fk_product\n",
        "            FOREIGN KEY(product_id)\n",
        "                REFERENCES products(product_id),\n",
        "        CONSTRAINT fk_seller\n",
        "            FOREIGN KEY(seller_id)\n",
        "                REFERENCES sellers(seller_id),\n",
        "        CONSTRAINT fk_order\n",
        "            FOREIGN KEY(order_id)\n",
        "                REFERENCES orders(order_id)\n",
        "    );\n",
        "    \"\"\"\n",
        "    create_products = \"\"\" \n",
        "    CREATE TABLE IF NOT EXISTS products (\n",
        "        product_id SERIAL PRIMARY KEY,\n",
        "        product_category VARCHAR(50),\n",
        "        product_weight_g NUMERIC CHECK (product_weight_g >=0)  \n",
        "    );\n",
        "    \"\"\"\n",
        "    create_sellers = \"\"\"\n",
        "    CREATE TABLE IF NOT EXISTS sellers (\n",
        "        seller_id SERIAL PRIMARY KEY,\n",
        "        seller_city VARCHAR(50),\n",
        "        seller_state VARCHAR(2)\n",
        "    );\n",
        "    \"\"\"\n",
        "\n",
        "    # Compl√©tez pour les autres tables\n",
        "    # N'oubliez pas les contraintes de cl√©s √©trang√®res !\n",
        "\n",
        "    with engine.connect() as conn:\n",
        "        conn.execute(text(create_customers))\n",
        "        conn.execute(text(create_orders))\n",
        "        conn.execute(text(create_sellers))\n",
        "        conn.execute(text(create_products))\n",
        "        conn.execute(text(create_order_items))\n",
        "        \n",
        "        # Ex√©cutez les autres CREATE TABLE\n",
        "        conn.commit()\n",
        "create_tables()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Traitements des donn√©es\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                          seller_id        seller_city seller_state\n",
            "0  3442f8959a84dea7ee197c632cb2df15           campinas           SP\n",
            "1  d1b65fc7debc3361ea86b5f14c68d2e2         mogi guacu           SP\n",
            "2  ce3ad9de960102d0677a81f5d0bb7b2d     rio de janeiro           RJ\n",
            "3  c0f3eea2e14555b6faeea3dd58c1b1c3          sao paulo           SP\n",
            "4  51a04a8a6bdcb23deccc82b0b80742cf  braganca paulista           SP\n"
          ]
        }
      ],
      "source": [
        "# Donn√©es vendeurs\n",
        "df_sellers = df_list['olist_sellers_dataset']\n",
        "df_sellers.isna().sum()\n",
        "df_sellers.drop(columns=['seller_zip_code_prefix'], inplace=True)\n",
        "print(df_sellers.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                        customer_id          customer_city customer_state\n",
            "0  06b8999e2fba1a1fbc88172c00ba8bc7                 franca             SP\n",
            "1  18955e83d337fd6b2def6b18a428ac77  sao bernardo do campo             SP\n",
            "2  4e7b3e00288586ebd08712fdd0374a03              sao paulo             SP\n",
            "3  b2b6027bc5c5109e529d4dc6358b12c3        mogi das cruzes             SP\n",
            "4  4f2d8ab171c80ec8364f7c12e35b23ad               campinas             SP\n"
          ]
        }
      ],
      "source": [
        "# Donn√©es clients\n",
        "df_customers = df_list['olist_customers_dataset']\n",
        "df_customers.isna().sum()\n",
        "df_customers.drop(columns=['customer_unique_id',\n",
        "                           'customer_zip_code_prefix'],\n",
        "                           inplace=True)\n",
        "print(df_customers.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                           order_id  order_item_id  \\\n",
            "0  00010242fe8c5a6d1ba2dd792cb16214              1   \n",
            "1  00018f77f2f0320c557190d7a144bdd3              1   \n",
            "2  000229ec398224ef6ca0657da4fc703e              1   \n",
            "3  00024acbcdf0a6daa1e931b038114c75              1   \n",
            "4  00042b26cf59d7ce69dfabb4e55b4fd9              1   \n",
            "\n",
            "                         product_id                         seller_id   price  \\\n",
            "0  4244733e06e7ecb4970a6e2683c13e61  48436dade18ac8b2bce089ec2a041202   58.90   \n",
            "1  e5f2d52b802189ee658865ca93d83a8f  dd7ddc04e1b6c2c614352b383efe2d36  239.90   \n",
            "2  c777355d18b72b67abbeef9df44fd0fd  5b51032eddd242adc84c38acab88f23d  199.00   \n",
            "3  7634da152a4610f1595efa32f14722fc  9d7a1d34a5052409006425275ba1c2b4   12.99   \n",
            "4  ac6c3623068f30de03045865e4e10089  df560393f3a51e74553ab94004ba5c87  199.90   \n",
            "\n",
            "   freight_value  \n",
            "0          13.29  \n",
            "1          19.93  \n",
            "2          17.87  \n",
            "3          12.79  \n",
            "4          18.14  \n"
          ]
        }
      ],
      "source": [
        "# Donn√©es objets command√©s\n",
        "\n",
        "df_order_items = df_list['olist_order_items_dataset']\n",
        "df_order_items.isna().sum()\n",
        "df_order_items.drop(columns=['shipping_limit_date'], inplace=True)\n",
        "print(df_order_items.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataframe Commandes\n",
        "df_orders = df_list['olist_orders_dataset']\n",
        "df_orders.isna().sum()\n",
        "\n",
        "# Suppression des colonnes non-utilis√©es\n",
        "df_orders.drop(columns=['order_approved_at', 'order_estimated_delivery_date',\n",
        "                        'order_delivered_carrier_date'], inplace=True)\n",
        "\n",
        "# Renommage de la colonne 'order_purchase_timestamp' en 'order_date'\n",
        "df_orders.rename(columns={'order_purchase_timestamp': 'order_date',\n",
        "                           'order_delivered_customer_date': 'order_delivered_date'},\n",
        "                           inplace=True)\n",
        "df_orders[df_orders[\"order_delivered_date\"].isnull()]\\\n",
        "         .groupby(\"order_status\").size()\n",
        "\n",
        "# Suppression des donn√©es manquantes dans la colonne 'order_delivered_date' \n",
        "df_orders = df_orders[df_orders['order_delivered_date'].notnull()]\n",
        "print(df_orders)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_36171/1806966647.py:17: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df_products['product_category'].fillna(\"uncategorized\", inplace=True)\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "\"['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_length_cm', 'product_height_cm', 'product_width_cm'] not found in axis\"",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[86]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m df_products.dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mproduct_weight_g\u001b[39m\u001b[33m\"\u001b[39m], inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# Suppression des colonnes inutiles\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mdf_products\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_name_lenght\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_description_lenght\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_photos_qty\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_length_cm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m                          \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_height_cm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_width_cm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(df_products)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Dataframe Commandes\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# df_orders = df_list['olist_orders_dataset']\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# df_orders.isna().sum()\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# print(df_orders.isna())\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/brief_dev_python_multianalyse/.venv/lib/python3.12/site-packages/pandas/core/frame.py:5588\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5441\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5442\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5449\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5450\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5451\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5452\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5453\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5586\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5587\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5588\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5590\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5591\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5592\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5594\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5595\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5596\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/brief_dev_python_multianalyse/.venv/lib/python3.12/site-packages/pandas/core/generic.py:4807\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4805\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4806\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4807\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4809\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4810\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/brief_dev_python_multianalyse/.venv/lib/python3.12/site-packages/pandas/core/generic.py:4849\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4847\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4848\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4849\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4850\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4852\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4853\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/brief_dev_python_multianalyse/.venv/lib/python3.12/site-packages/pandas/core/indexes/base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
            "\u001b[31mKeyError\u001b[39m: \"['product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_length_cm', 'product_height_cm', 'product_width_cm'] not found in axis\""
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Donn√©es produits Produits\n",
        "\n",
        "df_products = df_list['olist_products_dataset']\n",
        "df_products[df_products.isnull().any(axis=1)] # plusieurs donn√©es manquantes\n",
        "df_products.rename(columns={\"product_category_name\": \"product_category\"}, inplace=True) # mise √† jour du nom de colonne en vue d'un export sur la base de donn√©es cr√©e\n",
        "# Gestion des donn√©es manquantes dans la colonne 'product_category'\n",
        "df_products['product_category'].fillna(\"uncategorized\", inplace=True) \n",
        "df_products.dropna(subset=[\"product_weight_g\"], inplace=True)\n",
        "    # Suppression des colonnes inutiles\n",
        "df_products.drop(columns=['product_name_lenght', 'product_description_lenght', \n",
        "                          'product_photos_qty', 'product_length_cm', \n",
        "                          'product_height_cm', 'product_width_cm'], inplace=True)\n",
        "\n",
        "\n",
        "print(df_products)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBQ_BY-QT4dO"
      },
      "source": [
        "## Partie 3 : Requ√™tes SQL avanc√©es\n",
        "\n",
        "\n",
        "### üîç Analyses SQL √† impl√©menter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdl5RNOBUAV2"
      },
      "source": [
        "#### 1. Analyse RFM (R√©cence, Fr√©quence, Montant)\n",
        "```sql\n",
        "-- Votre d√©fi : Calculer les m√©triques RFM pour chaque client\n",
        "WITH customer_metrics AS (\n",
        "    SELECT\n",
        "        c.customer_id,\n",
        "        c.customer_state,\n",
        "        -- R√©cence : jours depuis dernier achat\n",
        "        -- Fr√©quence : nombre de commandes\n",
        "        -- Montant : total d√©pens√©\n",
        "        \n",
        "        -- Compl√©tez cette requ√™te CTE\n",
        "        \n",
        "    FROM customers c\n",
        "    JOIN orders o ON c.customer_id = o.customer_id\n",
        "    JOIN order_items oi ON o.order_id = oi.order_id\n",
        "    WHERE o.order_status = 'delivered'\n",
        "    GROUP BY c.customer_id, c.customer_state\n",
        ")\n",
        "\n",
        "-- Cr√©ez les segments RFM (Champions, Loyaux, √Ä risque, etc.)\n",
        "SELECT\n",
        "    customer_id,\n",
        "    customer_state,\n",
        "    recency_score,\n",
        "    frequency_score,\n",
        "    monetary_score,\n",
        "    CASE\n",
        "        WHEN recency_score >= 4 AND frequency_score >= 4 THEN 'Champions'\n",
        "        WHEN recency_score >= 3 AND frequency_score >= 3 THEN 'Loyal Customers'\n",
        "        -- Ajoutez les autres segments\n",
        "        ELSE 'Others'\n",
        "    END as customer_segment\n",
        "FROM customer_metrics;\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWF9rpZSUMp5"
      },
      "outputs": [],
      "source": [
        "#### 2. Analyse g√©ographique des ventes\n",
        "\n",
        "def geographic_sales_analysis():\n",
        "    \"\"\"\n",
        "    Analysez les performances par √©tat/r√©gion\n",
        "\n",
        "    Requ√™tes √† √©crire :\n",
        "    1. Top 10 des √©tats par CA\n",
        "    2. Croissance MoM par r√©gion\n",
        "    3. Taux de conversion par ville\n",
        "    4. Distance moyenne vendeur-acheteur\n",
        "    \"\"\"\n",
        "\n",
        "    query_top_states = \"\"\"\n",
        "    -- Votre requ√™te SQL ici\n",
        "    -- Utilisez des JOINs et GROUP BY\n",
        "    -- Calculez le CA, nombre de commandes, panier moyen\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(query_top_states, engine)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OE-UHLKY8-K"
      },
      "source": [
        "#### 3. Analyse temporelle et saisonnalit√©\n",
        "```sql\n",
        "-- D√©tectez les patterns saisonniers\n",
        "SELECT\n",
        "    EXTRACT(YEAR FROM order_date) as year,\n",
        "    EXTRACT(MONTH FROM order_date) as month,\n",
        "    EXTRACT(DOW FROM order_date) as day_of_week,\n",
        "    COUNT(*) as order_count,\n",
        "    SUM(price + freight_value) as total_revenue,\n",
        "    AVG(price + freight_value) as avg_order_value\n",
        "FROM orders o\n",
        "JOIN order_items oi ON o.order_id = oi.order_id\n",
        "WHERE order_status = 'delivered'\n",
        "GROUP BY ROLLUP(\n",
        "    EXTRACT(YEAR FROM order_date),\n",
        "    EXTRACT(MONTH FROM order_date),\n",
        "    EXTRACT(DOW FROM order_date)\n",
        ")\n",
        "ORDER BY year, month, day_of_week;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xq43e3mfZC8d"
      },
      "source": [
        "## Partie 4 : Analyse pr√©dictive avec SQL\n",
        "\n",
        "### üîÆ Mod√®les simples en SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bY5mfxFoaL2K"
      },
      "outputs": [],
      "source": [
        "#### 1. Pr√©diction de churn\n",
        "\n",
        "def churn_prediction_sql():\n",
        "    \"\"\"\n",
        "    Identifiez les clients √† risque de churn\n",
        "\n",
        "    Indicateurs :\n",
        "    - Pas d'achat depuis X jours\n",
        "    - Baisse de fr√©quence d'achat\n",
        "    - Diminution du panier moyen\n",
        "    - Changement de comportement g√©ographique\n",
        "    \"\"\"\n",
        "\n",
        "    churn_query = \"\"\"\n",
        "    WITH customer_activity AS (\n",
        "        -- Calculez les m√©triques d'activit√© r√©cente\n",
        "        -- Comparez avec l'historique du client\n",
        "        -- Scorez le risque de churn\n",
        "    )\n",
        "\n",
        "    SELECT\n",
        "        customer_id,\n",
        "        days_since_last_order,\n",
        "        order_frequency_trend,\n",
        "        monetary_trend,\n",
        "        churn_risk_score,\n",
        "        CASE\n",
        "            WHEN churn_risk_score > 0.7 THEN 'High Risk'\n",
        "            WHEN churn_risk_score > 0.4 THEN 'Medium Risk'\n",
        "            ELSE 'Low Risk'\n",
        "        END as churn_segment\n",
        "    FROM customer_activity;\n",
        "    \"\"\"\n",
        "\n",
        "    return pd.read_sql(churn_query, engine)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB2D1PDraVu4"
      },
      "source": [
        "#### 2. Recommandations produits\n",
        "```sql\n",
        "-- Market Basket Analysis simplifi√©\n",
        "WITH product_pairs AS (\n",
        "    SELECT\n",
        "        oi1.product_id as product_a,\n",
        "        oi2.product_id as product_b,\n",
        "        COUNT(*) as co_purchase_count\n",
        "    FROM order_items oi1\n",
        "    JOIN order_items oi2 ON oi1.order_id = oi2.order_id\n",
        "    WHERE oi1.product_id != oi2.product_id\n",
        "    GROUP BY oi1.product_id, oi2.product_id\n",
        "    HAVING COUNT(*) >= 10  -- Seuil minimum\n",
        ")\n",
        "\n",
        "SELECT\n",
        "    product_a,\n",
        "    product_b,\n",
        "    co_purchase_count,\n",
        "    co_purchase_count::float / total_a.count as confidence\n",
        "FROM product_pairs pp\n",
        "JOIN (\n",
        "    SELECT product_id, COUNT(*) as count\n",
        "    FROM order_items\n",
        "    GROUP BY product_id\n",
        ") total_a ON pp.product_a = total_a.product_id\n",
        "ORDER BY confidence DESC;\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbYkj8ItabH-"
      },
      "source": [
        "## Partie 5 : Int√©gration avec les APIs m√©t√©o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4CU6SNEfNXb"
      },
      "source": [
        "### üå§Ô∏è Croisement donn√©es m√©t√©o/ventes\n",
        "```python\n",
        "def weather_sales_correlation():\n",
        "    \"\"\"\n",
        "    Correlez vos donn√©es m√©t√©o du Notebook 1 avec les ventes\n",
        "    \n",
        "    Hypoth√®ses √† tester :\n",
        "    1. Les ventes de certaines cat√©gories augmentent-elles avec la pluie ?\n",
        "    2. Y a-t-il un impact de la temp√©rature sur les achats ?\n",
        "    3. Les livraisons sont-elles impact√©es par la m√©t√©o ?\n",
        "    \"\"\"\n",
        "    \n",
        "    # R√©cup√©rez les donn√©es m√©t√©o historiques pour les villes br√©siliennes\n",
        "    weather_query = \"\"\"\n",
        "    SELECT DISTINCT customer_city, customer_state\n",
        "    FROM customers\n",
        "    WHERE customer_state IN ('SP', 'RJ', 'MG', 'RS', 'SC')\n",
        "    ORDER BY customer_city;\n",
        "    \"\"\"\n",
        "    \n",
        "    cities = pd.read_sql(weather_query, engine)\n",
        "    \n",
        "    # Int√©grez avec l'API m√©t√©o\n",
        "    # Analysez les corr√©lations\n",
        "    \n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHG9k_5PfZXd"
      },
      "source": [
        "### üìä Dashboard g√©o-temporel\n",
        "```python\n",
        "def create_geotemporal_dashboard():\n",
        "    \"\"\"\n",
        "    Cr√©ez un dashboard interactif combinant :\n",
        "    - Carte des ventes par r√©gion\n",
        "    - √âvolution temporelle avec m√©t√©o\n",
        "    - Segments clients g√©olocalis√©s\n",
        "    - Pr√©dictions par zone g√©ographique\n",
        "    \"\"\"\n",
        "    pass\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsIuD-IVfnxW"
      },
      "source": [
        "---\n",
        "## üèÜ Livrables finaux\n",
        "\n",
        "### üìà Rapport d'analyse complet\n",
        "1. **Segmentation RFM (Recency, Frenquency, Monetary) ** : 5-7 segments avec caract√©ristiques\n",
        "2. **Analyse g√©ographique**  : Performances par r√©gion + recommandations\n",
        "3. **Pr√©dictions churn** : Liste des clients √† risque + actions\n",
        "4. **Recommandations produits** : Top 10 des associations\n",
        "5. **Impact m√©t√©o** : Corr√©lations significatives identifi√©es\n",
        "\n",
        "### üöÄ Pipeline automatis√©\n",
        "```python\n",
        "def automated_analysis_pipeline():\n",
        "    \"\"\"\n",
        "    Pipeline qui :\n",
        "    1. Se connecte √† la DB\n",
        "    2. Ex√©cute toutes les analyses\n",
        "    3. Met √† jour les segments clients\n",
        "    4. G√©n√®re le rapport automatiquement\n",
        "    5. Envoie des alertes si n√©cessaire\n",
        "    \"\"\"\n",
        "    pass\n",
        "```\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wynvmdtNftwf"
      },
      "source": [
        "## üéì Auto-√©valuation\n",
        "\n",
        "- [ ] **Connexion DB** : PostgreSQL fonctionnelle\n",
        "- [ ] **Requ√™tes complexes** : JOINs, CTEs, fonctions analytiques\n",
        "- [ ] **Gestion des erreurs** : Connexions robustes\n",
        "- [ ] **Performance** : Requ√™tes optimis√©es avec index\n",
        "- [ ] **Int√©gration** : SQL + Python + APIs\n",
        "- [ ] **Insights actionables** : Recommandations business claires\n",
        "\n",
        "### üîó Pr√©paration au Notebook 3\n",
        "Le prochain notebook portera sur NoSQL (MongoDB) avec des donn√©es de r√©seaux sociaux et d'IoT, en temps r√©el.\n",
        "\n",
        "### üí° Bases de donn√©es alternatives\n",
        "- **PlanetScale** : MySQL serverless gratuit\n",
        "- **MongoDB Atlas** : 512MB gratuit\n",
        "- **FaunaDB** : Base multi-mod√®le gratuite\n",
        "- **Hasura Cloud** : GraphQL + PostgreSQL"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
